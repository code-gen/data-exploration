{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocab and co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = {\n",
    "    'sep'   : u'\\u200b' + \"/-'´′‘…—−–\",\n",
    "    'keep'  : \"&\",\n",
    "    'remove': '?!.,，\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’™•°'\n",
    "}\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "\n",
    "    for p in PUNCTUATION['sep']:\n",
    "        x = x.replace(p, \" \")\n",
    "    for p in PUNCTUATION['keep']:\n",
    "        x = x.replace(p, \" %s \" % p)\n",
    "    for p in PUNCTUATION['remove']:\n",
    "        x = x.replace(p, \"\")\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb6501d537749dfb6d4de7049221ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Get all words', max=474, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(all_words) = 785676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baee10c0e2145a1979ef4395deed966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=785676), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c793fd269944ae3a28319091750cfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Constructing co-occurrence matrix', max=785676, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_all_words(data_source: str):\n",
    "    \"\"\"source: path to a directory containg files, or to a file containing paths\"\"\"\n",
    "\n",
    "    all_words = []\n",
    "\n",
    "    if os.path.isdir(data_source):\n",
    "        listing = sorted(glob.glob('%s/**/*.txt' % data_source, recursive=True))\n",
    "    else:  # regular file\n",
    "        listing = [l.strip() for l in open(data_source).readlines()]\n",
    "\n",
    "    word_predicate = lambda w: re.match(r'[\\w]+', w) and w not in stopWords\n",
    "\n",
    "    for file in tqdm(listing, desc=\"Get all words\"):\n",
    "        # TODO: use something smarter (spacy / nltk)\n",
    "        lines = [clean_text(l.strip().lower()) for l in open(file, \"rt\").readlines()]\n",
    "        all_words += [w for line in lines for w in line.split() if word_predicate(w)]\n",
    "\n",
    "    return all_words\n",
    "\n",
    "\n",
    "def create_vocab_counter(words):\n",
    "    vocab = Counter()\n",
    "    for w in tqdm(words):\n",
    "        vocab[w] += 1\n",
    "\n",
    "    # print(\"len(vocab) = %d\" % len(vocab))\n",
    "    return vocab\n",
    "\n",
    "# ---\n",
    "\n",
    "data_source = '../corpus/python-3.7.3-docs-text/'\n",
    "\n",
    "vocab_size = 20000\n",
    "window_size = 5\n",
    "\n",
    "all_words = get_all_words(data_source)\n",
    "print(\"len(all_words) = %d\" % len(all_words))\n",
    "\n",
    "top_words, top_freqs = zip(*create_vocab_counter(all_words).most_common()[:vocab_size])\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(top_words)}\n",
    "\n",
    "co_occur_mat = np.zeros((vocab_size, vocab_size), dtype=np.uint16)\n",
    "\n",
    "top_words_set = set(top_words)\n",
    "\n",
    "for i in tqdm(range(len(all_words)), desc='Constructing co-occurrence matrix'):\n",
    "    if all_words[i] not in top_words_set: continue\n",
    "\n",
    "    # window-search\n",
    "    for j in range(max(i - window_size, 0), min(i + window_size, len(all_words))):\n",
    "        if i == j or all_words[j] not in top_words_set: continue\n",
    "        co_occur_mat[word2idx[all_words[i]], word2idx[all_words[j]]] += 1\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 7367),\n",
       " ('bpo', 6578),\n",
       " ('object', 5628),\n",
       " ('module', 5437),\n",
       " ('return', 5330),\n",
       " ('new', 4751),\n",
       " ('function', 4750),\n",
       " ('file', 4579),\n",
       " ('value', 4395),\n",
       " ('class', 4367)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = pickle.load(open(\"/home/alex/Desktop/pydoc-window-5-size-20000.vocab\", \"rb\"))\n",
    "\n",
    "v.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_emb_file = '../embeddings/glove.840B.300d.txt.pickle'\n",
    "ft_emb_file = '../embeddings/pydoc-glove-fine-tuned-vocab-20000-window-5-iter-5000'\n",
    "vocab_file = '../embeddings/pydoc-vocab-20000-window-5.vocab'\n",
    "mat_file = '../embeddings/pydoc-vocab-20000-window-5.mat'\n",
    "\n",
    "ft_factor = 0.7\n",
    "pt_factor = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_emb = pickle.load(open(pt_emb_file, \"rb\"))\n",
    "orig_glove_emb = pickle.load(open(pt_emb_file, \"rb\"))\n",
    "\n",
    "ft_glove_emb_arr = pickle.load(open(ft_emb_file, \"rb\"))\n",
    "vocab = pickle.load(open(vocab_file, \"rb\"))\n",
    "mat = pickle.load(open(mat_file, 'rb'))\n",
    "\n",
    "ft_glove_emb = {w: ft_glove_emb_arr[i] for w, i in vocab.items()}\n",
    "\n",
    "x = 0\n",
    "for w in tqdm(ft_glove_emb, desc=\"Mixing embeddings (ft %.2f, pt %.2f)\" % (ft_factor, pt_factor)):\n",
    "    if w not in glove_emb:\n",
    "        glove_emb[w] = ft_glove_emb[w]\n",
    "        x += 1\n",
    "    else:\n",
    "        glove_emb[w] = ft_factor * ft_glove_emb[w] + pt_factor * glove_emb[w]\n",
    "        \n",
    "        \n",
    "print(x/len(ft_glove_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = lambda x, y: np.dot(x, y) #/ (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "w1 = 'return'\n",
    "w2 = 'function'\n",
    "\n",
    "print(sim(glove_emb[w1], glove_emb[w2]))\n",
    "print(sim(orig_glove_emb[w1], orig_glove_emb[w2]))\n",
    "\n",
    "# np.log(mat[vocab[w1], vocab[w2]]) - np.dot(glove_emb[w1], glove_emb[w2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
