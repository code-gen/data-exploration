{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import code\n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from collections import defaultdict\n",
    "import pydoc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../../raw-datasets/conala-corpus/conala-train.json'\n",
    "test_file  = '../../raw-datasets/conala-corpus/conala-test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    # python libs\n",
    "    \"os\"         : ['os.', 'from os import'],\n",
    "    \"(sh|ps)util\": ['shutil.', 'psutil.', 'from shutil import', 'from psutil import'],\n",
    "    \"sys\"        : ['sys.', 'from sys import'],\n",
    "    \"struct\"     : ['struct.', 'from struct import'],\n",
    "    \"subprocess\" : ['subprocess.', 'from subprocess import'],\n",
    "    \"date-time\"  : ['datetime.', 'calendar.', 'from datetime import', 'from calendar import'],\n",
    "    \"time\"       : ['time.', 'from time import'],\n",
    "    \"urllib\"     : ['urllib.', 'from urllib import'],\n",
    "    \"regex\"      : ['re.', 'from re import'],\n",
    "    \"itertools\"  : ['itertools.', 'from itertools import'],\n",
    "    \"random\"     : ['random.', 'from random import'],\n",
    "    \"requests\"   : ['request.', 'requests.'],\n",
    "    \"io\"         : ['open(', '.send('],\n",
    "    \"str-join\"   : [\".join(\"],\n",
    "\n",
    "    # 3rd party libs\n",
    "    \"django\"     : ['django'],\n",
    "    \"scipy\"      : ['scipy.', 'from scipy import'],\n",
    "    \"numpy\"      : ['np.', 'numpy.', 'from numpy import'],\n",
    "    \"pickle\"     : ['pickle.', 'from pickle import'],\n",
    "    \"pandas\"     : ['pd.', 'df[', 'df.', 'from pandas import', 'dataframe', 'pandas'],\n",
    "    \"matplotlib\" : ['plt.' 'fig.', 'ax.', 'import matplotlib', 'from matplotlib import'],\n",
    "    \"networkx\"   : ['nx.', 'from networkx import'],\n",
    "    \"gui\"        : ['gi.', 'wx.', 'tk.', 'dogtail'],\n",
    "    \"flask\"      : ['flask.', 'from flask import'],\n",
    "\n",
    "    # calls\n",
    "    \"functional\" : ['map(', 'filter(', 'reduce(', 'zip(', 'sum(', 'sorted('],\n",
    "    \"print\"      : ['print('],\n",
    "    \n",
    "    \"lambda\"     : ['lambda', 'anonymous'],\n",
    "\n",
    "    # list comprehension\n",
    "    \"list-comp\"  : [lambda x: x[0] == '[' and x[-1] == ']'],\n",
    "    \n",
    "    # generator\n",
    "    \"generator\"  : [lambda x: x[0] == '(' and x[-1] == ')'],\n",
    "}\n",
    "\n",
    "def get_unique(xs):\n",
    "    u, c = np.unique(xs, return_counts=True)\n",
    "    _c = np.argsort(-c)\n",
    "\n",
    "    return u[_c], c[_c]\n",
    "\n",
    "def query_by_key(data: Dict[str, Any], key):\n",
    "    return [data[i][key] for i in range(len(data))]\n",
    "\n",
    "def get_by_qid(data: Dict[str, Any], qid: int):\n",
    "    return list(filter(lambda ex: ex[\"question_id\"] == qid, data))\n",
    "\n",
    "def get_by_keywords(data: Dict[str, Any]):\n",
    "    xs = defaultdict(lambda: [])\n",
    " \n",
    "    for q in data:\n",
    "        found = False\n",
    "        \n",
    "        for ks in keywords:\n",
    "            for k in keywords[ks]:\n",
    "            \n",
    "                i = q[\"intent\"].lower()\n",
    "                ri = q[\"rewritten_intent\"].lower() if q[\"rewritten_intent\"] else None\n",
    "                s = q[\"snippet\"].lower()\n",
    "                \n",
    "                if isinstance(k, str):\n",
    "                    if (k in i) or (ri and k in ri) or (k in s):\n",
    "                        xs[ks].append(q)\n",
    "                        found = True\n",
    "                \n",
    "                # lambda predicate\n",
    "                else:\n",
    "                    if k(s):\n",
    "                        xs[ks].append(q)\n",
    "                        found = True\n",
    "            \n",
    "        # other keywords\n",
    "        if not found:\n",
    "            xs['other'].append(q)\n",
    "\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open(train_file, \"rt\"))\n",
    "test_data = json.load(open(test_file, \"rt\"))\n",
    "\n",
    "train_uids, train_cuids = get_unique(query_by_key(train_data, \"question_id\"))\n",
    "test_uids, test_cuids = get_unique(query_by_key(test_data, \"question_id\"))\n",
    "\n",
    "train_none_rewritten = [x for x in train_data if x['rewritten_intent'] is None]\n",
    "test_none_rewritten = [x for x in test_data if x['rewritten_intent'] is None]\n",
    "\n",
    "print(f\"[train] {train_uids.size} unique ids\")\n",
    "print(f\"[train] null-rewritten {round(100.0*len(train_none_rewritten)/len(train_data),3)}%\")\n",
    "print(f\"[test] {test_uids.size} unique ids\")\n",
    "print(f\"[test] null-rewritten {round(100.0*len(test_none_rewritten)/len(test_data),3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xs = get_by_keywords(train_data)\n",
    "test_xs = get_by_keywords(test_data)\n",
    "\n",
    "# print(\"[train]\")\n",
    "# for l, qs in sorted(train_xs.items(), key=lambda k : len(k[1]), reverse=True):\n",
    "#     print(f\"{l} -> {len(qs)}({round(100.0 * len(qs)/len(train_data), 3)}%)\")\n",
    "# print()\n",
    "\n",
    "# print(\"[test]\")\n",
    "# for l, qs in sorted(test_xs.items(), key=lambda k : len(k[1]), reverse=True):\n",
    "#     print(f\"{l} -> {len(qs)}({round(100.0 * len(qs)/len(test_data), 3)}%)\")\n",
    "\n",
    "print(f\"train-other: {round(100.0 * len(train_xs['other']) / len(train_data))}%\")\n",
    "print(f\"test-other: {round(100.0 * len(test_xs['other']) / len(test_data))}%\")\n",
    "\n",
    "s = sorted(train_xs.items(), key=lambda k : len(k[1]), reverse=True)\n",
    "labels = [x[0] for x in s]\n",
    "\n",
    "plt.figure(figsize=(int(0.67 * len(labels)), 6))\n",
    "\n",
    "plt.xticks(range(len(s)), labels, rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.yscale('symlog')\n",
    "\n",
    "plt.plot(range(len(s)), [len(x[1]) for x in s], 'r-', label='train')\n",
    "plt.plot(range(len(s)), [len(test_xs[k]) for k in labels], 'b-', label='test')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TranX intent preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTED_TOKEN_RE = re.compile(r\"(?P<quote>''|[`'\\\"])(?P<string>.*?)(?P=quote)\")\n",
    "\n",
    "\n",
    "def infer_slot_type(quote, value):\n",
    "    if quote == '`' and value.isidentifier():\n",
    "        return 'var'\n",
    "    return 'str'\n",
    "\n",
    "\n",
    "def canonicalize_intent(intent):\n",
    "    # handle the following special case: quote is `''`\n",
    "    marked_token_matches = QUOTED_TOKEN_RE.findall(intent)\n",
    "\n",
    "    slot_map = dict()\n",
    "    var_id = 0\n",
    "    str_id = 0\n",
    "    for match in marked_token_matches:\n",
    "        quote = match[0]\n",
    "        value = match[1]\n",
    "        quoted_value = quote + value + quote\n",
    "\n",
    "        # try:\n",
    "        #     # if it's a number, then keep it and leave it to the copy mechanism\n",
    "        #     float(value)\n",
    "        #     intent = intent.replace(quoted_value, value)\n",
    "        #     continue\n",
    "        # except:\n",
    "        #     pass\n",
    "\n",
    "        slot_type = infer_slot_type(quote, value)\n",
    "\n",
    "        if slot_type == 'var':\n",
    "            slot_name = 'var_%d' % var_id\n",
    "            var_id += 1\n",
    "            slot_type = 'var'\n",
    "        else:\n",
    "            slot_name = 'str_%d' % str_id\n",
    "            str_id += 1\n",
    "            slot_type = 'str'\n",
    "\n",
    "        # slot_id = len(slot_map)\n",
    "        # slot_name = 'slot_%d' % slot_id\n",
    "        # # make sure slot_name is also unicode\n",
    "        # slot_name = unicode(slot_name)\n",
    "\n",
    "        intent = intent.replace(quoted_value, slot_name)\n",
    "        slot_map[slot_name] = {'value': value.strip().encode().decode('unicode_escape', 'ignore'),\n",
    "                               'quote': quote,\n",
    "                               'type' : slot_type}\n",
    "\n",
    "    return intent, slot_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.sample(train_data, 1)[0]\n",
    "\n",
    "i = x['rewritten_intent'] if x['rewritten_intent'] is not None else x['intent']\n",
    "print(i)\n",
    "print(\"---\")\n",
    "\n",
    "i, s = canonicalize_intent(\"substitute 1 for `x`\")\n",
    "\n",
    "print(i)\n",
    "print(\"---\")\n",
    "\n",
    "pprint(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
