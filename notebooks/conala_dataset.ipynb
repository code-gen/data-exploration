{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNaLa Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import code\n",
    "from pprint import pprint\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from collections import defaultdict\n",
    "import pydoc\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# import nbimporter\n",
    "# from utils_nb import get_all_words_pred\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONALA_DIR = '../../raw-datasets/conala-corpus'\n",
    "train_file = f'{CONALA_DIR}/conala-train.json'\n",
    "test_file  = f'{CONALA_DIR}/conala-test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    # python libs\n",
    "    \"os\"         : ['os.', 'from os import'],\n",
    "    \"(sh|ps)util\": ['shutil.', 'psutil.', 'from shutil import', 'from psutil import'],\n",
    "    \"sys\"        : ['sys.', 'from sys import'],\n",
    "    \"struct\"     : ['struct.', 'from struct import'],\n",
    "    \"subprocess\" : ['subprocess.', 'from subprocess import'],\n",
    "    \"date-time\"  : ['datetime.', 'calendar.', 'from datetime import', 'from calendar import'],\n",
    "    \"time\"       : ['time.', 'from time import'],\n",
    "    \"urllib\"     : ['urllib.', 'from urllib import'],\n",
    "    \"regex\"      : ['re.', 'from re import'],\n",
    "    \"itertools\"  : ['itertools.', 'from itertools import'],\n",
    "    \"random\"     : ['random.', 'from random import'],\n",
    "    \"requests\"   : ['request.', 'requests.'],\n",
    "    \"io\"         : ['open(', '.send('],\n",
    "    \"str-join\"   : [\".join(\"],\n",
    "\n",
    "    # 3rd party libs\n",
    "    \"django\"     : ['django'],\n",
    "    \"scipy\"      : ['scipy.', 'from scipy import'],\n",
    "    \"numpy\"      : ['np.', 'numpy.', 'from numpy import'],\n",
    "    \"pickle\"     : ['pickle.', 'from pickle import'],\n",
    "    \"pandas\"     : ['pd.', 'df[', 'df.', 'from pandas import', 'dataframe', 'pandas'],\n",
    "    \"matplotlib\" : ['plt.' 'fig.', 'ax.', 'import matplotlib', 'from matplotlib import'],\n",
    "    \"networkx\"   : ['nx.', 'from networkx import'],\n",
    "    \"gui\"        : ['gi.', 'wx.', 'tk.', 'dogtail'],\n",
    "    \"flask\"      : ['flask.', 'from flask import'],\n",
    "\n",
    "    # calls\n",
    "    \"functional\" : ['map(', 'filter(', 'reduce(', 'zip(', 'sum(', 'sorted('],\n",
    "    \"print\"      : ['print('],\n",
    "    \n",
    "    \"lambda\"     : ['lambda', 'anonymous'],\n",
    "\n",
    "    # list comprehension\n",
    "    \"list-comp\"  : [lambda x: x[0] == '[' and x[-1] == ']'],\n",
    "    \n",
    "    # generator\n",
    "    \"generator\"  : [lambda x: x[0] == '(' and x[-1] == ')'],\n",
    "}\n",
    "\n",
    "def get_intent(ex):\n",
    "    return ex['rewritten_intent'] if ex['rewritten_intent'] is not None else ex['intent']\n",
    "\n",
    "def get_unique(xs):\n",
    "    u, c = np.unique(xs, return_counts=True)\n",
    "    _c = np.argsort(-c)\n",
    "\n",
    "    return u[_c], c[_c]\n",
    "\n",
    "def query_by_key(data: Dict[str, Any], key):\n",
    "    return [data[i][key] for i in range(len(data))]\n",
    "\n",
    "def get_by_qid(data: Dict[str, Any], qid: int):\n",
    "    return list(filter(lambda ex: ex[\"question_id\"] == qid, data))\n",
    "\n",
    "def get_by_keywords(data: Dict[str, Any]):\n",
    "    xs = defaultdict(lambda: [])\n",
    " \n",
    "    for q in data:\n",
    "        found = False\n",
    "        \n",
    "        for ks in keywords:\n",
    "            for k in keywords[ks]:\n",
    "            \n",
    "                i = q[\"intent\"].lower()\n",
    "                ri = q[\"rewritten_intent\"].lower() if q[\"rewritten_intent\"] else None\n",
    "                s = q[\"snippet\"].lower()\n",
    "                \n",
    "                if isinstance(k, str):\n",
    "                    if (k in i) or (ri and k in ri) or (k in s):\n",
    "                        xs[ks].append(q)\n",
    "                        found = True\n",
    "                \n",
    "                # lambda predicate\n",
    "                else:\n",
    "                    if k(s):\n",
    "                        xs[ks].append(q)\n",
    "                        found = True\n",
    "            \n",
    "        # other keywords\n",
    "        if not found:\n",
    "            xs['other'].append(q)\n",
    "\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(os.path.join(CONALA_DIR, 'conala-train.json')) as fp:\n",
    "    data += json.load(fp)\n",
    "    \n",
    "with open(os.path.join(CONALA_DIR, 'conala-test.json')) as fp:\n",
    "    data += json.load(fp)\n",
    "    \n",
    "anno, code = [], []\n",
    "for d in data:\n",
    "    a = d['rewritten_intent'] if d['rewritten_intent'] is not None else d['intent']\n",
    "    a = a.replace('\\n', ' ')\n",
    "    \n",
    "    if len(a.split()) > 10:\n",
    "        continue\n",
    "        \n",
    "    ok = True\n",
    "    for x in a.split():\n",
    "        if len(x) > 15:\n",
    "            ok = False\n",
    "            break\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    c = d['snippet']\n",
    "    c = c.replace('\\n', ' ')\n",
    "    \n",
    "    if len(c.split()) > 10:\n",
    "        continue\n",
    "        \n",
    "    ok = True\n",
    "    for x in c.split():\n",
    "        if len(x) > 15:\n",
    "            ok = False\n",
    "            break\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    anno.append(a)\n",
    "    code.append(c)\n",
    "    \n",
    "assert len(anno) == len(code)\n",
    "\n",
    "with open(os.path.join(CONALA_DIR, 'all.anno'), 'wt') as fp:\n",
    "    for a in anno:\n",
    "        fp.write(f'{a}\\n')\n",
    "        \n",
    "with open(os.path.join(CONALA_DIR, 'all.code'), 'wt') as fp:\n",
    "    for a in code:\n",
    "        fp.write(f'{a}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l ../../raw-datasets/conala-corpus/all.anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l ../../raw-datasets/conala-corpus/all.code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open(train_file, \"rt\"))\n",
    "test_data = json.load(open(test_file, \"rt\"))\n",
    "\n",
    "train_uids, train_cuids = get_unique(query_by_key(train_data, \"question_id\"))\n",
    "test_uids, test_cuids = get_unique(query_by_key(test_data, \"question_id\"))\n",
    "\n",
    "train_none_rewritten = [x for x in train_data if x['rewritten_intent'] is None]\n",
    "test_none_rewritten = [x for x in test_data if x['rewritten_intent'] is None]\n",
    "\n",
    "print(f\"[train] {train_uids.size} unique ids\")\n",
    "print(f\"[train] null-rewritten {round(100.0*len(train_none_rewritten)/len(train_data),3)}%\")\n",
    "print(f\"[test] {test_uids.size} unique ids\")\n",
    "print(f\"[test] null-rewritten {round(100.0*len(test_none_rewritten)/len(test_data),3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(train_data + test_data)\n",
    "d.snippet.apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xs = get_by_keywords(train_data)\n",
    "test_xs = get_by_keywords(test_data)\n",
    "\n",
    "# print(\"[train]\")\n",
    "# for l, qs in sorted(train_xs.items(), key=lambda k : len(k[1]), reverse=True):\n",
    "#     print(f\"{l} -> {len(qs)}({round(100.0 * len(qs)/len(train_data), 3)}%)\")\n",
    "# print()\n",
    "\n",
    "# print(\"[test]\")\n",
    "# for l, qs in sorted(test_xs.items(), key=lambda k : len(k[1]), reverse=True):\n",
    "#     print(f\"{l} -> {len(qs)}({round(100.0 * len(qs)/len(test_data), 3)}%)\")\n",
    "\n",
    "print(f\"train-other: {round(100.0 * len(train_xs['other']) / len(train_data))}%\")\n",
    "print(f\"test-other: {round(100.0 * len(test_xs['other']) / len(test_data))}%\")\n",
    "\n",
    "s = sorted(train_xs.items(), key=lambda k : len(k[1]), reverse=True)\n",
    "labels = [x[0] for x in s]\n",
    "\n",
    "plt.figure(figsize=(int(0.67 * len(labels)), 6))\n",
    "\n",
    "plt.xticks(range(len(s)), labels, rotation=45, fontsize=18)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.yscale('symlog')\n",
    "\n",
    "plt.plot(range(len(s)), [len(x[1]) for x in s], 'r-', label='train', linewidth=4.0)\n",
    "plt.plot(range(len(s)), [len(test_xs[k]) for k in labels], 'b-', label='test', linewidth=5.0)\n",
    "      \n",
    "    \n",
    "\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.grid()\n",
    "plt.title('CoNaLa example distribution', size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check intent-corpus coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_emb = pickle.load(open('../../embeddings/glove.6B.200d.txt.pickle', 'rb'))\n",
    "\n",
    "c = get_all_words_pred(\n",
    "    corpus='../../corpus/python-stackoverflow/question_words_clean.pickle',\n",
    "    vocab_size=10000,\n",
    "    word_predicate=lambda w: w in pt_emb,\n",
    "    min_freq=1, pt_emb=pt_emb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coverage for all words in intents that are also present in embeddings (i.e. skip var names etc)\n",
    "\n",
    "xs = [get_intent(train_data[i]).split() for i in range(len(train_data))]\n",
    "xs = list(reduce(lambda a, b : list(set(a + b)), xs, []))\n",
    "\n",
    "i = t = 0\n",
    "ws = []\n",
    "for w in xs:\n",
    "    if w not in pt_emb: continue\n",
    "    \n",
    "    if w in c:\n",
    "        i += 1\n",
    "    else:\n",
    "        ws += [w]\n",
    "        \n",
    "    t += 1\n",
    "    \n",
    "print(\"Intent-corpus coverage\", round(i/t, 5))\n",
    "\n",
    "ws[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
