{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Django Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from utils_nb import get_all_words_pred\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import argparse\n",
    "import code\n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple, Counter\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_file = '../../raw-datasets/django/all.code'\n",
    "anno_file = '../../raw-datasets/django/all.anno'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_lines = [l.strip() for l in open(code_file, \"rt\").readlines()]\n",
    "anno_lines = [l.strip() for l in open(anno_file, \"rt\").readlines()]\n",
    "\n",
    "print(len(code_lines))\n",
    "\n",
    "assert len(code_lines) == len(anno_lines)\n",
    "\n",
    "example = namedtuple(\"Example\", [\"code\", \"label\"])\n",
    "\n",
    "data = list(map(lambda x : example(*x), zip(code_lines, anno_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "for i in range(len(data[:1])):\n",
    "    print(data[i].label)\n",
    "    print(data[i].code)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = {\n",
    "    'sep'   : u'\\u200b' + \"/-'´′‘…—−–\",\n",
    "    'keep'  : \"&\",\n",
    "    'remove': '?!.,，\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’™•°'\n",
    "}\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "\n",
    "    for p in PUNCTUATION['sep']:\n",
    "        x = x.replace(p, \" \")\n",
    "    for p in PUNCTUATION['keep']:\n",
    "        x = x.replace(p, \" %s \" % p)\n",
    "    for p in PUNCTUATION['remove']:\n",
    "        x = x.replace(p, \"\")\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.sample(anno_lines, 1)[0].split()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "anno_vocab = Counter()\n",
    "\n",
    "for a in tqdm(anno_lines):\n",
    "    for w in filter(lambda x : x not in stopWords, tokenizer.tokenize(a)):\n",
    "        anno_vocab[w.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "words, counts = zip(*anno_vocab.most_common(n))\n",
    "\n",
    "plt.figure(figsize=(int(0.64 * n),6))\n",
    "plt.xticks(range(len(words)), words, rotation=60, fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "# plt.yscale('symlog')\n",
    "plt.plot(range(len(words)), counts, 'b-', linewidth=5)\n",
    "plt.grid()\n",
    "plt.title('Django top-%d utterances in intent' % n, size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing = [l.strip() for l in open('../../corpus/python-docs/listing.txt').readlines()]\n",
    "all_words = Counter()\n",
    "\n",
    "for l in tqdm(listing, desc=\"Get all words\"):\n",
    "    file_contents = [clean_text(l.strip().lower()) for l in open(l, \"rt\").readlines()]\n",
    "    \n",
    "    for line in file_contents:\n",
    "        for w in line.split():\n",
    "            if re.match(r'[\\w]+', w) and w not in stopWords:\n",
    "                all_words[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_coverage(data, vocab) -> (Counter, Counter):\n",
    "    \"\"\"\n",
    "    How much of 'data' is in 'vocab'\n",
    "    \"\"\"\n",
    "    \n",
    "    oov = Counter() # out-of-vocab\n",
    "    inv = Counter() # in-vocab\n",
    "    oov_uniq_num = inv_uniq_num = 0.0\n",
    "    oov_all_num = inv_all_num = 0.0\n",
    "    \n",
    "    for w in tqdm(data):\n",
    "        if w in vocab:\n",
    "            inv[w] = data[w]\n",
    "            inv_uniq_num += 1\n",
    "            inv_all_num += data[w]\n",
    "        else:\n",
    "            oov[w] = data[w]\n",
    "            oov_uniq_num += 1\n",
    "            oov_all_num += data[w]\n",
    "    \n",
    "    \n",
    "    assert inv_all_num + oov_all_num == sum(data.values())\n",
    "    \n",
    "    cov_uniq = 100.0 * round(inv_uniq_num / len(data), 5)\n",
    "    cov_all = 100.0 * round(inv_all_num / (inv_all_num + oov_all_num), 5)\n",
    "    \n",
    "    print(\"coverage (unique): %.3f%%\" % cov_uniq)\n",
    "    print(\"coverage (all text): %.3f%%\" % cov_all)\n",
    "    \n",
    "    return oov, inv\n",
    "\n",
    "\n",
    "oov, inv = get_vocab_coverage(anno_vocab, all_words)\n",
    "\n",
    "'assign' in all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check intent-corpus coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt_emb = pickle.load(open('../../embeddings/glove.6B.200d.txt.pickle', 'rb'))\n",
    "\n",
    "c = get_all_words_pred(\n",
    "    corpus='../../corpus/python-stackoverflow/question_words_clean.pickle',\n",
    "    vocab_size=8000,\n",
    "    word_predicate=lambda w: w in pt_emb,\n",
    "    min_freq=100, pt_emb=pt_emb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coverage for all words in intents that are also present in embeddings (i.e. skip var names etc)\n",
    "\n",
    "n = len(anno_vocab)\n",
    "d = set(anno_vocab.keys()) - set(c.keys())\n",
    "i = set(anno_vocab.keys()).intersection(set(c.keys()))\n",
    "\n",
    "x = t = 0\n",
    "ws = []\n",
    "\n",
    "for w in anno_vocab:\n",
    "    if w not in pt_emb: continue\n",
    "    \n",
    "    if w in c:\n",
    "        x += 1\n",
    "    else:\n",
    "        ws += [w]\n",
    "        \n",
    "    t += 1\n",
    "    \n",
    "print(\"Intent-corpus coverage\", round(x/t, 5))\n",
    "\n",
    "ws[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
