{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Django Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import namedtuple, Counter\n",
    "import builtins\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "# import nbimporter\n",
    "# from utils_nb import get_all_words_pred\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_file = '../../raw-datasets/django/all.code'\n",
    "anno_file = '../../raw-datasets/django/all.anno'\n",
    "\n",
    "code_lines = [l.strip() for l in open(code_file, \"rt\").readlines()]\n",
    "anno_lines = [l.strip() for l in open(anno_file, \"rt\").readlines()]\n",
    "\n",
    "assert len(code_lines) == len(anno_lines)\n",
    "print('dataset size:', len(code_lines))\n",
    "\n",
    "df = pd.DataFrame({'anno': anno_lines, 'code': code_lines})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.anno.apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.code.apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, c = df.iloc[np.random.randint(len(df))].values\n",
    "print('>', a)\n",
    "print('>', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = {\n",
    "    'sep'   : u'\\u200b' + \"/-'´′‘…—−–\",\n",
    "    'keep'  : \"&\",\n",
    "    'remove': '?!.,，\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’™•°'\n",
    "}\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "\n",
    "    for p in PUNCTUATION['sep']:\n",
    "        x = x.replace(p, \" \")\n",
    "    for p in PUNCTUATION['keep']:\n",
    "        x = x.replace(p, \" %s \" % p)\n",
    "    for p in PUNCTUATION['remove']:\n",
    "        x = x.replace(p, \"\")\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "anno_vocab = Counter()\n",
    "\n",
    "for a in tqdm(anno_lines):\n",
    "    toks = set(tokenizer.tokenize(a)) - stopWords\n",
    "    for tok in toks:\n",
    "        anno_vocab[tok.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "words, counts = zip(*anno_vocab.most_common(n))\n",
    "\n",
    "plt.figure(figsize=(int(0.5 * n), 6))\n",
    "plt.xticks(range(len(words)), words, rotation=60, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "# plt.yscale('symlog')\n",
    "plt.plot(range(len(words)), counts, 'b-', linewidth=2)\n",
    "plt.title('Django top-%d utterances in intent' % n, size=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical vocabulary\n",
    "\n",
    "This might help overcome the high level of verbosity in the annotation vocabulary.\n",
    "A hierarchical vocabulary follows the principle of sketch representations (i.e. abstract away the specifics).\n",
    "\n",
    "**Level 1**\n",
    "- Python reserved keywords\n",
    "- class / variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_KEYWORDS = set(dir(builtins) + [\n",
    "    \"and\", \"assert\", \"break\", \"class\", \"continue\", \"def\", \"del\", \"elif\",\n",
    "    \"else\", \"except\", \"exec\", \"finally\", \"for\", \"from\", \"global\", \"if\",\n",
    "    \"import\", \"in\", \"is\", \"lambda\", \"not\", \"or\", \"pass\", \"print\", \"raise\",\n",
    "    \"return\", \"try\", \"while\", \"yield\", \"None\", \"self\"\n",
    "])  # len = 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tokens_by(main_vocab: Counter, criteria: Callable):\n",
    "    return [(word, count) for (word, count) in main_vocab.items() if criteria(word)]\n",
    "\n",
    "# find_tokens_by(anno_vocab, lambda w: w in RESERVED_KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing = [l.strip() for l in open('../../corpus/python-docs/listing.txt').readlines()]\n",
    "all_words = Counter()\n",
    "\n",
    "for l in tqdm(listing, desc=\"Get all words\"):\n",
    "    file_contents = [clean_text(l.strip().lower()) for l in open(l, \"rt\").readlines()]\n",
    "    \n",
    "    for line in file_contents:\n",
    "        for w in line.split():\n",
    "            if re.match(r'[\\w]+', w) and w not in stopWords:\n",
    "                all_words[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_coverage(data, vocab) -> (Counter, Counter):\n",
    "    \"\"\"\n",
    "    How much of 'data' is in 'vocab'\n",
    "    \"\"\"\n",
    "    \n",
    "    oov = Counter() # out-of-vocab\n",
    "    inv = Counter() # in-vocab\n",
    "    oov_uniq_num = inv_uniq_num = 0.0\n",
    "    oov_all_num = inv_all_num = 0.0\n",
    "    \n",
    "    for w in tqdm(data):\n",
    "        if w in vocab:\n",
    "            inv[w] = data[w]\n",
    "            inv_uniq_num += 1\n",
    "            inv_all_num += data[w]\n",
    "        else:\n",
    "            oov[w] = data[w]\n",
    "            oov_uniq_num += 1\n",
    "            oov_all_num += data[w]\n",
    "    \n",
    "    \n",
    "    assert inv_all_num + oov_all_num == sum(data.values())\n",
    "    \n",
    "    cov_uniq = 100.0 * round(inv_uniq_num / len(data), 5)\n",
    "    cov_all = 100.0 * round(inv_all_num / (inv_all_num + oov_all_num), 5)\n",
    "    \n",
    "    print(\"coverage (unique): %.3f%%\" % cov_uniq)\n",
    "    print(\"coverage (all text): %.3f%%\" % cov_all)\n",
    "    \n",
    "    return oov, inv\n",
    "\n",
    "\n",
    "oov, inv = get_vocab_coverage(anno_vocab, all_words)\n",
    "\n",
    "'assign' in all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check intent-corpus coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_emb = pickle.load(open('../../embeddings/glove.6B.200d.txt.pickle', 'rb'))\n",
    "\n",
    "c = get_all_words_pred(\n",
    "    corpus='../../corpus/python-stackoverflow/question_words_clean.pickle',\n",
    "    vocab_size=8000,\n",
    "    word_predicate=lambda w: w in pt_emb,\n",
    "    min_freq=100, pt_emb=pt_emb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coverage for all words in intents that are also present in embeddings (i.e. skip var names etc)\n",
    "\n",
    "n = len(anno_vocab)\n",
    "d = set(anno_vocab.keys()) - set(c.keys())\n",
    "i = set(anno_vocab.keys()).intersection(set(c.keys()))\n",
    "\n",
    "x = t = 0\n",
    "ws = []\n",
    "\n",
    "for w in anno_vocab:\n",
    "    if w not in pt_emb: continue\n",
    "    \n",
    "    if w in c:\n",
    "        x += 1\n",
    "    else:\n",
    "        ws += [w]\n",
    "        \n",
    "    t += 1\n",
    "    \n",
    "print(\"Intent-corpus coverage\", round(x/t, 5))\n",
    "\n",
    "ws[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
