{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff4d253823c76782432402d45e2b90587b907fb2"
   },
   "source": [
    "## Finding Similar Stack Overflow Questions: Comparing Centroid Method with Doc2Vec\n",
    "\n",
    "\n",
    "This notebook will attempt to answer the question __How can we embed sentences in such a way that similar questions will appear closer to each other as measured by cosine distance?__ by comparing two embedding methods __Centroid Method__ and __Doc2Vec__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import FastText\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "base_dir = '../corpus/python-stackoverflow/'\n",
    "\n",
    "qdf = pd.read_csv(\n",
    "    os.path.join(base_dir, 'Questions.csv'), \n",
    "    encoding = \"ISO-8859-1\", nrows=30000, usecols=['Id', 'Title', 'Body']\n",
    ")\n",
    "\n",
    "adf = pd.read_csv(\n",
    "    os.path.join(base_dir, 'Answers.csv'), \n",
    "    encoding = \"ISO-8859-1\", nrows=30000, usecols=['Id', 'Body']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "906ef06f075087ed5e55c7389674412ba763fee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Q: <p>I am using the Photoshop's javascript API to find the fonts in a given PSD.</p>\n",
      "\n",
      "<p>Given a font name returned by the API, I want to find the actual physical font file that that font name corresponds to on the disc.</p>\n",
      "\n",
      "<p>This is all happening in a python program running on OSX so I guess I'm looking for one of:</p>\n",
      "\n",
      "<ul>\n",
      "<li>Some Photoshop javascript</li>\n",
      "<li>A Python function</li>\n",
      "<li>An OSX API that I can call from python</li>\n",
      "</ul>\n",
      " \n",
      "\n",
      ">>> A: <p>open up a terminal (Applications-&gt;Utilities-&gt;Terminal) and type this in:</p>\r\n",
      "\r\n",
      "<pre><code>locate InsertFontHere<br></code></pre>\r\n",
      "\r\n",
      "<p>This will spit out every file that has the name you want.</p>\r\n",
      "\r\n",
      "<p>Warning: there may be alot to wade through.</p> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('>>> Q:', qdf.iloc[0, 2], \"\\n\")\n",
    "print('>>> A:', adf.iloc[0, 1], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97b48209e063f56a31206be254614ed4c573e790"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "2bebded00eb88856ae289d8e27baa81e7a6801f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bc34b88dc245a387306525c602ff1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b5461e0362487e9f7b9625414cc03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def beautify(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    return ' '.join([t.text for t in soup.find_all('p')]) # concat all p tags\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "question_list = qdf['Body'].progress_apply(beautify).values.tolist()\n",
    "answer_list = adf['Body'].progress_apply(beautify).values.tolist()\n",
    "\n",
    "question_words = [list(filter(lambda w : w not in stop_words, s)) for s in sent_to_words(question_list)]\n",
    "answer_words   = [list(filter(lambda w : w not in stop_words, s)) for s in sent_to_words(answer_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "50b88ec3796927e67ae59671bc13cc28396052ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['using',\n",
       "  'photoshop',\n",
       "  'javascript',\n",
       "  'api',\n",
       "  'find',\n",
       "  'fonts',\n",
       "  'given',\n",
       "  'psd',\n",
       "  'given',\n",
       "  'font',\n",
       "  'name',\n",
       "  'returned',\n",
       "  'api',\n",
       "  'want',\n",
       "  'find',\n",
       "  'actual',\n",
       "  'physical',\n",
       "  'font',\n",
       "  'file',\n",
       "  'font',\n",
       "  'name',\n",
       "  'corresponds',\n",
       "  'disc',\n",
       "  'happening',\n",
       "  'python',\n",
       "  'program',\n",
       "  'running',\n",
       "  'osx',\n",
       "  'guess',\n",
       "  'looking',\n",
       "  'one'],\n",
       " ['open',\n",
       "  'terminal',\n",
       "  'applications',\n",
       "  'utilities',\n",
       "  'terminal',\n",
       "  'type',\n",
       "  'spit',\n",
       "  'every',\n",
       "  'file',\n",
       "  'name',\n",
       "  'want',\n",
       "  'warning',\n",
       "  'may',\n",
       "  'alot',\n",
       "  'wade'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(question_words[0], answer_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c19d7852f9e84e157d2903774ee4aad9f8c8b99c"
   },
   "source": [
    "## Sentence Embedding \n",
    "As a first approach, I will be using a so called __centroid method__ to dervie the sentence embeddings (taken from this research paper http://www2.aueb.gr/users/ion/docs/BioNLP_2016.pdf). It derives sentence embeddings as the sum of individual word embeddings in a sentece weighted by their tf-idf score, and divided by the sum of these tf-idf scores.  For the sake of simplicity, I'm going to be compare just two alternatives for word embeddings Word2Vec and FastText. I'll be using gensim implementations of both.\n",
    "\n",
    "## Word2Vec\n",
    "Word2Vec model learns vector representation of a word by either predicting the context around it (skip-gram), or predicting a word based on its context (CBoW). The most important parameters to specify here are the size of embbedding vector and the size of context window. The number of dimensions is usually between 100-300, with 128 being a standard choice for a lot of applications. Context window depends on the nature of text and embbeddings that you want to get. We'll start with context of 5, and see if the embbedings make sense. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2283facaf0ef42a0d13d625415f0ff264e9bfbf"
   },
   "outputs": [],
   "source": [
    "#Instantiating the model\n",
    "n = 50\n",
    "model = Word2Vec(filtered_questions, size = n, window = 8)\n",
    "\n",
    "#Training model using questions corpora\n",
    "model.train(filtered_questions, total_examples=len(filtered_questions), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db7fb288145cd48d9efa64b0c11ee018623fd152"
   },
   "source": [
    "Let's inspect the results by looking at the most similar words (vectors) of a word 'array' and 'database''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecfe822d97fc72b0d457516ab2a05c4c132fb63a"
   },
   "outputs": [],
   "source": [
    "#Let's see how it worked\n",
    "word_vectors = model.wv\n",
    "\n",
    "print('Words similar to \"array\" are: ', word_vectors.most_similar(positive='array'))\n",
    "print('Words similar to \"database\" are: ', word_vectors.most_similar(positive='database'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "780745ccb9b0fd02bcefb32760bfde3db044fe9e"
   },
   "source": [
    "Looks like word2vec knows that array is related to list, matrix, and is commonly used in context of slicing. For 'database', we can see that mode has lerned the abbreviation of db, and the related topics like tables and sqlite. In general, these results should be good enough to construct the sentence embeddings out of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f9ddd705b519dd5d133e2aca7dfae396d423600"
   },
   "source": [
    "## FastText\n",
    "The main difference of FastText from Word2Vec is that it uses sub-word information (i.e character n-grams). While it brings additional utility to the embeddings, it also considerably slows down the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7980a123b38e246191dc18ea827d2ed3d7f1d0e7"
   },
   "outputs": [],
   "source": [
    "ft_model = FastText(filtered_questions, size=n, window=8, min_count=5, workers=2,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0333451b4c1514cb1e47a3b61b2f6f812e815a2"
   },
   "outputs": [],
   "source": [
    "print('Words similar to \"array\" are: ', ft_model.wv.most_similar('array'))\n",
    "print('Words similar to \"database\" are: ', ft_model.wv.most_similar('database'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e716f81a05444b935a70cf7dd74f776c9f097135"
   },
   "source": [
    "Here we can see that FastText has produced different vector embeddings. 'Array' now is close to the words which also contain the ngram 'array' and 'database' is close to different ngrams of the word database plus some variations of database tools. \n",
    "\n",
    "We can clearly see the difference between embbedding methods - Word2Vec puts the words which occur in the same context closer in the vector space, while FastText does the same but also allows to incorporate less frequent words into this vector space. Use of n-grams really does play a key role in word embbedings and hence, **I will proceed with using FastText embbeddings** as a basis for sentence embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b602b437ab44f90bc2a3d47fedaf38ceccf9d846"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98efa9676b2515445cb5ee2b98be26a75d4f187e"
   },
   "outputs": [],
   "source": [
    "#dct = Dictionary(filtered_questions)  # fit dictionary\n",
    "#corpus = [dct.doc2bow(line) for line in filtered_questions]  # convert corpus to BoW format\n",
    "#tfidf_model = TfidfModel(corpus)  # fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ad852e394420e1e98b13d9c76085e8d2e3b1a3f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(question_list)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a4c41a3fa84f3b024dd4b9d4719ae7899ffa972"
   },
   "outputs": [],
   "source": [
    "#To proprely work with scikit's vectorizer\n",
    "merged_questions = [' '.join(question) for question in filtered_questions]\n",
    "document_names = ['Doc {:d}'.format(i) for i in range(len(merged_questions))]\n",
    "\n",
    "def get_tfidf(docs, ngram_range=(1,1), index=None):\n",
    "    vect = TfidfVectorizer(stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf = vect.fit_transform(docs).todense()\n",
    "    return pd.DataFrame(tfidf, columns=vect.get_feature_names(), index=index).T\n",
    "\n",
    "tfidf = get_tfidf(merged_questions, ngram_range=(1,1), index=document_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14b952dec5bfbd3ac21bfc960d3e705f3cfdbc6a"
   },
   "source": [
    "### Centroid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd01992d62b205fa749cefe55552a153792c7cfc"
   },
   "outputs": [],
   "source": [
    "def get_sent_embs(emb_model):\n",
    "    sent_embs = []\n",
    "    for desc in range(len(filtered_questions)):\n",
    "        sent_emb = np.zeros((1, n))\n",
    "        if len(filtered_questions[desc]) > 0:\n",
    "            sent_emb = np.zeros((1, n))\n",
    "            div = 0\n",
    "            model = emb_model\n",
    "            for word in filtered_questions[desc]:\n",
    "                if word in model.wv.vocab and word in tfidf.index:\n",
    "                    word_emb = model.wv[word]\n",
    "                    weight = tfidf.loc[word, 'Doc {:d}'.format(desc)]\n",
    "                    sent_emb = np.add(sent_emb, word_emb * weight)\n",
    "                    div += weight\n",
    "                else:\n",
    "                    div += 1e-13 #to avoid dividing by 0\n",
    "        if div == 0:\n",
    "            print(desc)\n",
    "\n",
    "        sent_emb = np.divide(sent_emb, div)\n",
    "        sent_embs.append(sent_emb.flatten())\n",
    "    return sent_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2526b714ebd3e7d57bf1fd3801b166a5c5184a"
   },
   "outputs": [],
   "source": [
    "ft_sent = get_sent_embs(emb_model = ft_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1617fc5085def2c1241b4d3b127e9af3789f80f6"
   },
   "source": [
    "## Finding Similar Questions\n",
    "Now we have sentence embeddings which in theory should reflect the similarity of some questions. To check if this assumption is valid, let's pick a question and find top 5 similar questions (knearest neighbours) as measured by cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66696ffbc347158a145eaa9c3c80fe3e1352b7f6"
   },
   "outputs": [],
   "source": [
    "def get_n_most_similar(interest_index, embeddings, n):\n",
    "    \"\"\"\n",
    "    Takes the embedding vector of interest, the list with all embeddings, and the number of similar questions to \n",
    "    retrieve.\n",
    "    Outputs the disctionary IDs and distances\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=n, metric='cosine').fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "    similar_indices = indices[interest_index][1:]\n",
    "    similar_distances = distances[interest_index][1:]\n",
    "    return similar_indices, similar_distances\n",
    "\n",
    "def print_similar(interest_index, embeddings, n):\n",
    "    \"\"\"\n",
    "    Convenience function for visual analysis\n",
    "    \"\"\"\n",
    "    closest_ind, closest_dist = get_n_most_similar(interest_index, embeddings, n)\n",
    "    print('Question %s \\n \\n is most similar to these %s questions: \\n' % (question_list[interest_index], n))\n",
    "    for question in closest_ind:\n",
    "        print('ID ', question, ': ',question_list[question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d7afc4ef1fd1b359f8356479a330bee0b70e7cc"
   },
   "outputs": [],
   "source": [
    "print_similar(42, ft_sent, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b123d1321f18f30c5e201c92c7fb7f8a12dbfb39"
   },
   "source": [
    "Results are quite interesting. All of the questions are about some kind of text processing. Not exactly repeating questions, but we are definitely onto something. Possible explanation for a weak perfromance is that questions are too long and the final embedding is influenced by too much noise. My hope was that tf-idf score would counteract this, but apparently this is not the case. However, for shorter texts, this method works quite well. \n",
    "\n",
    "Next appraoch will be a more complicated (in terms of theory, not implementation) model called __Doc2Vec__. \n",
    "\n",
    "## Doc2Vec\n",
    "Doc2Vec improves on simple averaging method by training a 'document' vector along the word vectors. As in Word2Vec there are two algortihms available to train the model, but I will be using the 'distributed memory' (that's why dm=1 in my model). It trains a model which predicts a word based on its context, by averaging the context word and paragraph ID vectors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cf50b70016380b5dfff7b99f25e7f8a7466606a"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(filtered_questions)]\n",
    "model = Doc2Vec(documents, vector_size=n, window=8, min_count=5, workers=2, dm = 1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3dd01d7332bae20c128734ebb03a2426e9d27da"
   },
   "outputs": [],
   "source": [
    "print(question_list[42], ' \\nis similar to \\n')\n",
    "print([question_list[similar[0]] for similar in model.docvecs.most_similar(42)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f892b54e7a6da6f549b007c9f340ecff6beab4f6"
   },
   "source": [
    "Results are less than impressive. Some results are about string manipulations or SQL, but Doc2Vec has failed to capture the main meaning of the reference question. \n",
    "\n",
    "From the current analysis I can conclude that with current parameters, __Centroid Method outperforms Doc2Vec__. Here's is another example of similar questions being close to each-other under the Centroid Method Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "14c526ac12171ae5c11bdd7d3fefcc938ac99864"
   },
   "outputs": [],
   "source": [
    "print_similar(101, ft_sent, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe8915eacb9e8da39d6efa9d41620d3d5f76e823"
   },
   "source": [
    "Next steps to improve embeddings would be to:\n",
    "* Add more tags to Doc2Vec which, in theory, would push questions with similar tags closer together\n",
    "* Concatenate question headers and code parts with question text \n",
    "* Experiment with more questions (now we are training on a limited dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
